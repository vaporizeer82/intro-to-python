{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lec_act_2_functions.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture goals\n",
    "\n",
    "1. (more) Dictionaries for data encapsulation\n",
    "2. Functions for functionality encapsulation\n",
    "\n",
    "Lab slides: https://docs.google.com/presentation/d/1ykwwcQ0onMvAjUxfJmKl9tbo-rJPdB5pRwDEmpDsd-g/edit?usp=sharing\n",
    "\n",
    "## Functions: \n",
    "Functions enable encapsulation of, well, functionality.\n",
    "\n",
    "They're also a useful mental tool for organizing and structuring your thoughts on how to solve a given problem\n",
    "1. Clearly define a bit of code that takes in some inputs, does some computation, then outputs some data\n",
    "2. Makes it easier to test that code with different inputs\n",
    "3. Practicalities: Prevents one of the most common sources of errors - re-using variable names\n",
    "\n",
    "It's almost never wrong to encapsulate a bit of code in a function. It can slow down (a tiny bit) computation time, but can greatly reduce debugging time, so it's usually worth it.\n",
    "\n",
    "Python's function syntax is beautifully designed to make it easy to set default values for parameters and pass back as much data as you want. We'll see more of that later; for this assignment we'll use the power of dictionaries to pass back \"labeled\" data.\n",
    "\n",
    "In this lecture activity you're essentially going to copy over the code you did in the previous lecture activity, right-shift it to place it in the function, replace the **test_** variable name with the input parameter to the function, then add a return to return the answer. This is a pretty common approach for turning code into a function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access all numpy functions as np.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Stats on a list\n",
    "\n",
    "### Calculate stats on a list\n",
    "\n",
    "TODO: in calc_stats_from_list function \n",
    "- Calculate the mean of the negative and positive values\n",
    "- Count the total number of negative/positive values\n",
    "- Store the values in a dictionary\n",
    "\n",
    "This function calculates the given stats from the list that is passed in. There is test code below this function.\n",
    "\n",
    "TODO: \n",
    " - Step 1 - copy your code from lecture activity 1 into the function. Right shift it so it is indented properly\n",
    " - Step 2 - change **test_list_one** to be **in_list** - the input parameter of the function\n",
    " - Step 3 - return the dictionary **dict_save_stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_stats_from_list(in_list):\n",
    "    \"\"\" Calculate mean of positive numbers, mean of negatives numbers\n",
    "    Separate the list into positive and negative numbers. Calculate the mean of each. Return those means, along with\n",
    "     how many positive/negative numbers there were\n",
    "    @param in_list : any list type\n",
    "    @return - A dictionary with the desired stats\"\"\"\n",
    "\n",
    "    # These are the stats we're calculating. This is more elegant/useful than creating four variables - it keeps all\n",
    "    #  of the values in the same place and assigns a meaningful label (key) to them\n",
    "    dict_save_stats = {\"Mean positive\": 0.0, \"Mean negative\": -0.0, \"Count positive\": 0, \"Count negative\": 0}\n",
    "\n",
    "    # TODO: \n",
    "    #   Copy your code from lecture activity 1 here. Don't forget to change the name of test_list_one to be in_list\n",
    "    count_negative=0\n",
    "    count_positive=0\n",
    "    neg_list=[]\n",
    "    pos_list=[]\n",
    "\n",
    "    for item in in_list:\n",
    "        if item < 0:\n",
    "            count_negative = count_negative+1\n",
    "            neg_list.append(item)\n",
    "        elif item > 0:\n",
    "            count_positive = count_positive+1\n",
    "            pos_list.append(item)\n",
    "\n",
    "   \n",
    "    mean_positive=sum(pos_list)/len(pos_list)\n",
    "\n",
    "    mean_negative=sum(neg_list)/len(neg_list)\n",
    "\n",
    "    dict_save_stats[\"Count negative\"]=count_negative\n",
    "    dict_save_stats[\"Count positive\"]=count_positive\n",
    "    dict_save_stats[\"Mean negative\"]=mean_negative\n",
    "    dict_save_stats[\"Mean positive\"]=mean_positive\n",
    "\n",
    "    # TODO Do the return here\n",
    "    \n",
    "    return dict_save_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Test code for list\n",
    "\n",
    "Create the arrays and test your function using those test arrays. Here's another advantage of functions - you can create test data for yourself to make sure the code is working right. Encapsulating the code in a function means you don't \n",
    "1. Accidentally change the code when switching from the test data to the real data\n",
    "2. You can make more than one test \n",
    "3. You can run the tests more than once/all the time to double check that you didn't \"break\" the code\n",
    "\n",
    "TODO: \n",
    "- Fill in the calc_stats_from_list function above\n",
    "- Run the cell below - it will print out if your values are incorrect\n",
    "\n",
    "Note that, below, we'll test this code one last time with randomly generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All array tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "test_list_one = [-0.75, -0.25, 1.0 / 3.0, 2.0 / 3.0, 3.0 / 3.0]\n",
    "test_list_res = calc_stats_from_list(test_list_one)\n",
    "\n",
    "b_tests_passed = True\n",
    "if not np.isclose(test_list_res[\"Mean positive\"], 2.0 / 3.0):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean positive is not correct, should be {2.0/3.0}, got {test_list_res['Mean positive']}\")\n",
    "\n",
    "if not np.isclose(test_list_res[\"Mean negative\"], -0.5):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean negative is not correct, should be -0.5, got {test_list_res['Mean negative']}\")\n",
    "\n",
    "if test_list_res[\"Count positive\"] != 3:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 3, got {test_list_res['Count positive']}\")\n",
    "\n",
    "if test_list_res[\"Count negative\"] != 2:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 2, got {test_list_res['Count negative']}\")\n",
    "\n",
    "if b_tests_passed:\n",
    "    print(\"All array tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>list</pre></strong> passed! âœ¨</p>"
      ],
      "text/plain": [
       "list results: All test cases passed!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Question 2: Doing it again with a numpy array\n",
    "\n",
    "## Fill in calc_stats_from_nparray\n",
    "\n",
    "For this function, assume the input is a numpy array.\n",
    "\n",
    "TODO: Same as the previous question, but this time copy in the numpy array code you wrote in lecture activity 1. Again:\n",
    "- NO **if** statements or **for** loops - do this all with numpy operations\n",
    "\n",
    "As before, test code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_stats_from_nparray(in_nparray):\n",
    "    \"\"\" Calculate mean of positive numbers, mean of negatives numbers\n",
    "    Separate the list into positive and negative numbers. Calculate the mean of each. Return those means, along with\n",
    "     how many positive/negative numbers there were\n",
    "    @param in_nparray : numpy array\n",
    "    @return - A dictionary with the desired stats\"\"\"\n",
    "\n",
    "    # TODO: Copy in the code, change the numpy array name to be the input to this function, and return the dictionary\n",
    "    my_array = np.array(in_nparray)\n",
    "    negs = my_array<0\n",
    "    pos = my_array>0\n",
    "\n",
    "    all_negs = my_array[negs]\n",
    "    all_pos = my_array[pos]\n",
    "\n",
    "    dict_save_stats_np = {}\n",
    "\n",
    "    dict_save_stats_np[\"Count negative\"] = np.count_nonzero(all_negs)\n",
    "    dict_save_stats_np[\"Count positive\"] = np.count_nonzero(all_pos)\n",
    "    dict_save_stats_np[\"Mean negative\"] = np.mean(all_negs)\n",
    "    dict_save_stats_np[\"Mean positive\"] = np.mean(all_pos)\n",
    "    return dict_save_stats_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Test code for numpy array\n",
    "\n",
    "This will print out if your function above is returning incorrect values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All numpy array tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_nparray_one = np.array(test_list_one)  # Convert the previous test list to a numpy array\n",
    "test_list_res = calc_stats_from_nparray(test_nparray_one)\n",
    "\n",
    "b_tests_passed = True\n",
    "if not np.isclose(test_list_res[\"Mean positive\"], 2.0 / 3.0):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean positive is not correct, should be {2.0/3.0}, got {test_list_res['Mean positive']}\")\n",
    "\n",
    "if not np.isclose(test_list_res[\"Mean negative\"], -0.5):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean negative is not correct, should be -0.5, got {test_list_res['Mean negative']}\")\n",
    "\n",
    "if test_list_res[\"Count positive\"] != 3:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 3, got {test_list_res['Count positive']}\")\n",
    "\n",
    "if test_list_res[\"Count negative\"] != 2:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 2, got {test_list_res['Count negative']}\")\n",
    "\n",
    "if b_tests_passed:\n",
    "    print(\"All numpy array tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>nparray</pre></strong> passed! ðŸŽ‰</p>"
      ],
      "text/plain": [
       "nparray results: All test cases passed!"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"nparray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulating the data slice \n",
    "\n",
    "In this problem we'll encapsulate the fancy slice you did in lab 2 where you get out the x,y,z columns for the wrist torque data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 1320)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "try:\n",
    "    fname = \"Data/proxy_pick_data.csv\"\n",
    "    pick_data = np.loadtxt(fname, dtype=\"float\", delimiter=\",\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found {fname}\")\n",
    "\n",
    "\n",
    "# TODO - copy over your code from Lab 1 here\n",
    "pick_channel_data =pick_data[:,:-1]\n",
    "print(pick_channel_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_channel_data(all_data, n_picks, start_index, n_time_steps, n_total_dims, n_dims):\n",
    "    \"\"\" Get the data for just one channel (eg, wrist torque)\n",
    "    @param all_data - the pick_channel_data numpy array\n",
    "    @param n_picks - number of picks (number of rows in all_data)\n",
    "    @param start_index - where to start getting data from \n",
    "    @param n_time_steps - number of time steps\n",
    "    @param n_total_dims - what the skip value is - the total number of channels\n",
    "    @param n_dims - total number of dimensions to use (1, 2, or 3)\n",
    "    @return Return array should be n_picks X (n_timesteps * n_dims)\"\"\"\n",
    "\n",
    "    # TODO Your slice code goes here. Note that I kept most of the variable names the same, so you should only have\n",
    "    #  to change the wrist torque specific ones\n",
    "   \n",
    "    wrist_torque_data = np.zeros((n_picks, n_time_steps * n_dims))\n",
    "    wrist_torque_data[0:n_picks,0:n_time_steps] = pick_channel_data[:, start_index::n_total_dims]\n",
    "    wrist_torque_data[:, 1::3] = pick_channel_data[:, start_index+1::n_total_dims]\n",
    "    wrist_torque_data[:, 2::3] = pick_channel_data[:, start_index+2::n_total_dims]\n",
    "\n",
    "\n",
    "    print(wrist_torque_data.shape)\n",
    "    print(wrist_torque_data[0,-1])\n",
    "    return wrist_torque_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 120)\n",
      "-0.077284199\n"
     ]
    }
   ],
   "source": [
    "# Just for this problem I'm \"hard-wiring\" in these values - this is because I want to test the function\n",
    "# with known values\n",
    "n_picks = 660\n",
    "wt_start_index = 3  # Wrist torque starts at 3\n",
    "n_time_steps = 40\n",
    "n_total_dims = 33\n",
    "\n",
    "# Note - I know the wrist torque data has 3 dimensions (x,y,z)\n",
    "wrist_torque_data = get_channel_data(pick_channel_data, \n",
    "                                     n_picks=n_picks, \n",
    "                                     start_index=wt_start_index,\n",
    "                                     n_time_steps=n_time_steps,\n",
    "                                     n_total_dims=n_total_dims,\n",
    "                                     n_dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.69000015e+01 -4.37332067e-01  4.80904677e+00 -5.57173931e+00\n",
      " -4.33966910e-01 -1.72362242e-01 -7.72841990e-02 -2.62883353e+00\n",
      " -9.73721790e+00 -1.24977326e+00 -3.15093994e+00 -1.06048584e+00\n",
      " -7.78198242e-01  4.29791992e+02  0.00000000e+00  3.22799988e+01\n",
      " -1.61369193e+00 -3.28723884e+00  9.39963531e+00 -1.75476074e-01\n",
      "  7.32421875e-01  3.81469730e-02  2.59864014e+02  0.00000000e+00\n",
      "  2.95900002e+01 -2.54503632e+00 -9.80904388e+00 -1.22343707e+00\n",
      " -3.77655029e+00 -1.12915039e+00  7.01904297e-01  2.09880005e+02\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(pick_channel_data[0,-34:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>data_function</pre></strong> passed! ðŸš€</p>"
      ],
      "text/plain": [
       "data_function results: All test cases passed!"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"data_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Hours and collaborators\n",
    "Required for every assignment - fill out before you hand-in.\n",
    "\n",
    "Listing names and websites helps you to document who you worked with and what internet help you received in the case of any plagiarism issues. You should list names of anyone (in class or not) who has substantially helped you with an assignment - or anyone you have *helped*. You do not need to list TAs.\n",
    "\n",
    "Listing hours helps us track if the assignments are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# List of names (creates a set)\n",
    "worked_with_names = {\"none\"}\n",
    "# List of URLS TCW3 (creates a set)\n",
    "websites = {\"none\"}\n",
    "# Approximate number of hours, including lab/in-class time\n",
    "hours = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>hours_collaborators</pre></strong> passed! ðŸ™Œ</p>"
      ],
      "text/plain": [
       "hours_collaborators results: All test cases passed!"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"hours_collaborators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Submit through gradescope, lecture activity 2 Functions. Be sure to read the info on the autograder before submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running your submission against local test cases...\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "c:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\zmq\\_future.py:679: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\r\n  self._get_loop()\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save your notebook first, then run this cell to export your submission.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m grader\u001b[38;5;241m.\u001b[39mexport(pdf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, run_tests\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\otter\\check\\utils.py:184\u001b[0m, in \u001b[0;36mgrading_mode_disabled\u001b[1;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_grading_mode:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\otter\\check\\utils.py:166\u001b[0m, in \u001b[0;36mincompatible_with.<locals>.incompatible\u001b[1;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\otter\\check\\utils.py:217\u001b[0m, in \u001b[0;36mlogs_event.<locals>.event_logger\u001b[1;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_event(event_type, success\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     ret \u001b[38;5;241m=\u001b[39m LoggedEventReturnValue(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\otter\\check\\utils.py:213\u001b[0m, in \u001b[0;36mlogs_event.<locals>.event_logger\u001b[1;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03mRuns a method, catching any errors and logging the call. Returns the unwrapped return value\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03mof the wrapped function.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     ret: Optional[LoggedEventReturnValue[T]] \u001b[38;5;241m=\u001b[39m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_event(event_type, success\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[1;32mc:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\otter\\check\\notebook.py:523\u001b[0m, in \u001b[0;36mNotebook.export\u001b[1;34m(self, nb_path, export_path, pdf, filtering, pagebreaks, files, display_link, force_save, run_tests)\u001b[0m\n\u001b[0;32m    520\u001b[0m         display(HTML(out_html))\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pdf_created \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nbmeta_config\u001b[38;5;241m.\u001b[39mrequire_no_pdf_confirmation:\n\u001b[1;32m--> 523\u001b[0m     continue_export()\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     display_pdf_confirmation_widget(\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nbmeta_config\u001b[38;5;241m.\u001b[39mexport_pdf_failure_message, continue_export)\n",
      "File \u001b[1;32mc:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\otter\\check\\notebook.py:505\u001b[0m, in \u001b[0;36mNotebook.export.<locals>.continue_export\u001b[1;34m()\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_tests:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning your submission against local test cases...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 505\u001b[0m     results \u001b[38;5;241m=\u001b[39m grade_zip_file(zip_path, nb_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tests_dir)\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour submission received the following results when run against \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailable test cases:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m indent(results\u001b[38;5;241m.\u001b[39msummary(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_link:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# create and display output HTML\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\otter\\check\\utils.py:103\u001b[0m, in \u001b[0;36mgrade_zip_file\u001b[1;34m(zip_path, nb_arcname, tests_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(results\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mstderr:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(results\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    106\u001b[0m     results \u001b[38;5;241m=\u001b[39m dill\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: c:\\Users\\yeasshhhh\\anaconda3\\Lib\\site-packages\\zmq\\_future.py:679: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\r\n  self._get_loop()\r\n"
     ]
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "data_function": {
     "name": "data_function",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(wrist_torque_data.shape[0] == pick_data.shape[0])\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(wrist_torque_data.shape[1] == 3 * 40)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(wrist_torque_data[0, 0], -0.268183058, atol=0.001))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(wrist_torque_data[0, -1], -0.-0.077284199, atol=0.001))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "hours_collaborators": {
     "name": "hours_collaborators",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(not \"not filled out\" in worked_with_names)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(not \"not filled out\" in websites)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(hours > 0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "list": {
     "name": "list",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res = calc_stats_from_list([-1, 2, -3, 4, -5])\n>>> assert np.isclose(res[\"Mean positive\"], 3.0)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> res = calc_stats_from_list([-1, 2, -3, 4, -5])\n>>> assert np.isclose(res[\"Mean negative\"], -3.0)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> res = calc_stats_from_list([-1, 2, -3, 4, -5])\n>>> assert np.isclose(res[\"Count positive\"], 2)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> res = calc_stats_from_list([-1, 2, -3, 4, -5])\n>>> assert np.isclose(res[\"Count negative\"], 3)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "nparray": {
     "name": "nparray",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> test_data = np.array([-1, 2, -3, 4, -5])\n>>> res = calc_stats_from_nparray(test_data)\n>>> assert np.isclose(res[\"Mean positive\"], 3.0)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_data = np.array([-1, 2, -3, 4, -5])\n>>> res = calc_stats_from_nparray(test_data)\n>>> assert np.isclose(res[\"Mean negative\"], -3.0)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_data = np.array([-1, 2, -3, 4, -5])\n>>> res = calc_stats_from_nparray(test_data)\n>>> assert np.isclose(res[\"Count positive\"], 2)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_data = np.array([-1, 2, -3, 4, -5])\n>>> res = calc_stats_from_nparray(test_data)\n>>> assert np.isclose(res[\"Count negative\"], 3)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
